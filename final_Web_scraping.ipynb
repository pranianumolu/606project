{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install webdriver_manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0riTRcPJ3ws",
        "outputId": "b744da07-ba10-4aa0-8f04-4b6b13ccae8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting webdriver_manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver_manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver_manager) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver_manager) (2025.4.26)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, webdriver_manager\n",
            "Successfully installed python-dotenv-1.1.0 webdriver_manager-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wFayq-AS7Ea",
        "outputId": "21495bd0-a14f-448d-f7ed-1da3b343cf09"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPe7WoAwS-Gw",
        "outputId": "00867b95-f385-4742-d05b-6af67920711f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fake-useragent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVt8-PHATCTJ",
        "outputId": "446d077b-814b-4736-f792-4fa0791fc2c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GibPajp4JzYz",
        "outputId": "ab38b59e-8edc-4a7c-86b5-9ea3c566a7a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.32.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn nltk tensorflow keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EoSxpff-Av8",
        "outputId": "e257d0a5-8955-4b98-aed5-c45a571463e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivdN8tJ7C_9s",
        "outputId": "e1f44a79-5c65-497f-f074-0b3a5973e37b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')  # Optional: If you need lemmatization\n",
        "nltk.download('omw-1.4')  # Optional: For WordNet\n",
        "\n",
        "print(\"NLTK resources downloaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHcdWwdbCpcf",
        "outputId": "9ebeeae0-3160-42ac-e56e-7274f566e201"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK resources downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Function to scrape Freelancer.com job postings\n",
        "def scrape_freelancer_jobs(keywords, num_pages=3):\n",
        "    ua = UserAgent()\n",
        "    headers = {\"User-Agent\": ua.random}\n",
        "    jobs_list = []\n",
        "\n",
        "    if isinstance(keywords, str):  # Convert single keyword to a list\n",
        "        keywords = [keywords]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f\"https://www.freelancer.com/jobs/{keyword}/{page}/\"\n",
        "            response = requests.get(url, headers=headers)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Blocked or failed request on page {page} for keyword: {keyword}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            jobs = soup.find_all(\"div\", class_=\"JobSearchCard-item\")\n",
        "\n",
        "            for job in jobs:\n",
        "                try:\n",
        "                    title = job.find(\"a\", class_=\"JobSearchCard-primary-heading-link\").text.strip()\n",
        "                    description = job.find(\"p\", class_=\"JobSearchCard-primary-description\").text.strip()\n",
        "                    budget = job.find(\"div\", class_=\"JobSearchCard-secondary-price\").text.strip() if job.find(\"div\", class_=\"JobSearchCard-secondary-price\") else \"N/A\"\n",
        "\n",
        "                    jobs_list.append({\"Keyword\": keyword, \"Title\": title, \"Description\": description, \"Budget\": budget})\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(jobs_list)\n",
        "\n",
        "# Scrape Freelancer.com for multiple keywords\n",
        "df = scrape_freelancer_jobs([\"data-science\", \"machine-learning\", \"deep-learning\", \"nlp\", \"ai\", \"computer-vision\", \"data-engineering\",\n",
        "    \"data-visualization\", \"data-mining\", \"statistical-analysis\", \"predictive-modeling\", \"big-data\",\n",
        "    \"software-development\", \"mobile-app-development\", \"game-development\", \"ar-vr-development\",\n",
        "    \"web-development\", \"full-stack-development\", \"frontend-development\", \"backend-development\",\n",
        "    \"react-js\", \"angular-js\", \"vue-js\", \"node-js\", \"flutter\", \"swift\", \"android-development\",\n",
        "    \"java-development\", \"python-development\", \"c-sharp\", \"golang\", \"php\", \"ruby-on-rails\",\n",
        "    \"blockchain\", \"crypto\", \"smart-contracts\", \"cybersecurity\", \"ethical-hacking\", \"penetration-testing\",\n",
        "    \"network-security\", \"cloud-security\", \"malware-analysis\", \"digital-forensics\", \"aws\", \"azure\",\n",
        "\n",
        "], num_pages=3)\n",
        "df.to_csv(\"freelancer_jobs.csv\", index=False)\n",
        "print(\"Scraping complete! Data saved to freelancer_jobs.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TEIHwxxASLx",
        "outputId": "690e6adf-e722-4da8-89a7-07f3763658e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete! Data saved to freelancer_jobs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Function to scrape Freelancer.com job postings\n",
        "def scrape_freelancer_jobs(keywords, num_pages=3):\n",
        "    ua = UserAgent()\n",
        "    headers = {\"User-Agent\": ua.random}\n",
        "    jobs_list = []\n",
        "\n",
        "    if isinstance(keywords, str):  # Convert single keyword to a list\n",
        "        keywords = [keywords]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f\"https://www.freelancer.com/jobs/{keyword}/{page}/\"\n",
        "            response = requests.get(url, headers=headers)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Blocked or failed request on page {page} for keyword: {keyword}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            jobs = soup.find_all(\"div\", class_=\"JobSearchCard-item\")\n",
        "\n",
        "            for job in jobs:\n",
        "                try:\n",
        "                    title = job.find(\"a\", class_=\"JobSearchCard-primary-heading-link\").text.strip()\n",
        "                    description = job.find(\"p\", class_=\"JobSearchCard-primary-description\").text.strip()\n",
        "                    budget = job.find(\"div\", class_=\"JobSearchCard-secondary-price\").text.strip() if job.find(\"div\", class_=\"JobSearchCard-secondary-price\") else \"N/A\"\n",
        "\n",
        "                    jobs_list.append({\"Keyword\": keyword, \"Title\": title, \"Description\": description, \"Budget\": budget})\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(jobs_list)\n",
        "\n",
        "# Scrape Freelancer.com for multiple keywords\n",
        "df = scrape_freelancer_jobs([\"google-cloud\", \"devops\", \"kubernetes\", \"docker\", \"terraform\", \"system-administration\",\n",
        "    \"server-administration\", \"ci-cd\", \"electrical-engineering\", \"mechanical-engineering\",\n",
        "    \"civil-engineering\", \"biotechnology\", \"bioinformatics\", \"robotics\", \"automotive-engineering\",\n",
        "    \"renewable-energy\", \"structural-engineering\", \"chemical-engineering\", \"aerospace-engineering\",\n",
        "    \"environmental-engineering\", \"petroleum-engineering\", \"embedded-systems\", \"iot\",\n",
        "    \"firmware-development\", \"pcb-design\", \"cad-design\", \"finance\", \"business-analysis\",\n",
        "    \"market-research\", \"business-plans\", \"financial-modeling\", \"investment-research\",\n",
        "    \"accounting\", \"tax-preparation\", \"valuation\", \"mergers-acquisitions\", \"risk-management\",\n",
        "    \"bookkeeping\", \"legal\", \"contract-drafting\", \"intellectual-property\", \"patent-filing\",\n",
        "    \"compliance-consulting\", \"corporate-law\", \"immigration-services\", \"real-estate-law\",\n",
        "    \"hr-consulting\", \"business-consulting\", \"seo\", \"digital-marketing\", \"affiliate-marketing\",\n",
        "    \"lead-generation\", \"social-media-marketing\", \"email-marketing\", \"branding\",\n",
        "    \"growth-hacking\", \"ppc\", \"sales-funnel\", \"public-relations\", \"conversion-rate-optimization\"\n",
        "], num_pages=3)\n",
        "df.to_csv(\"freelancer_jobs_2.csv\", index=False)\n",
        "print(\"Scraping complete! Data saved to freelancer_jobs.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQPc-AkTDRJ5",
        "outputId": "042053a0-fbca-487c-c334-84e82dfdd5b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete! Data saved to freelancer_jobs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Function to scrape Freelancer.com job postings\n",
        "def scrape_freelancer_jobs(keywords, num_pages=3):\n",
        "    ua = UserAgent()\n",
        "    headers = {\"User-Agent\": ua.random}\n",
        "    jobs_list = []\n",
        "\n",
        "    if isinstance(keywords, str):  # Convert single keyword to a list\n",
        "        keywords = [keywords]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f\"https://www.freelancer.com/jobs/{keyword}/{page}/\"\n",
        "            response = requests.get(url, headers=headers)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Blocked or failed request on page {page} for keyword: {keyword}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            jobs = soup.find_all(\"div\", class_=\"JobSearchCard-item\")\n",
        "\n",
        "            for job in jobs:\n",
        "                try:\n",
        "                    title = job.find(\"a\", class_=\"JobSearchCard-primary-heading-link\").text.strip()\n",
        "                    description = job.find(\"p\", class_=\"JobSearchCard-primary-description\").text.strip()\n",
        "                    budget = job.find(\"div\", class_=\"JobSearchCard-secondary-price\").text.strip() if job.find(\"div\", class_=\"JobSearchCard-secondary-price\") else \"N/A\"\n",
        "\n",
        "                    jobs_list.append({\"Keyword\": keyword, \"Title\": title, \"Description\": description, \"Budget\": budget})\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(jobs_list)\n",
        "\n",
        "# Scrape Freelancer.com for multiple keywords\n",
        "df = scrape_freelancer_jobs([\"influencer-marketing\", \"cold-calling\", \"advertising\", \"media-buying\", \"graphic-design\",\n",
        "    \"logo-design\", \"illustration\", \"animation\", \"video-editing\", \"motion-graphics\",\n",
        "    \"ui-ux-design\", \"infographics\", \"product-packaging\", \"3d-modeling\", \"cad-design\",\n",
        "    \"architectural-visualization\", \"print-design\", \"nft-art\", \"industrial-design\",\n",
        "    \"content-writing\", \"copywriting\", \"resume-writing\", \"technical-writing\", \"business-writing\",\n",
        "    \"research-writing\", \"speech-writing\", \"grant-writing\", \"book-writing\", \"proposal-writing\",\n",
        "    \"press-releases\", \"medical-writing\", \"transcription\", \"translation\", \"proofreading\",\n",
        "    \"subtitling\", \"typing-jobs\", \"customer-service\", \"virtual-assistant\", \"data-entry\",\n",
        "    \"call-center-support\", \"email-handling\", \"data-annotation\", \"legal-transcription\",\n",
        "    \"ecommerce\", \"dropshipping\", \"shopify-development\", \"woocommerce\", \"etsy-marketing\",\n",
        "    \"amazon-fba\", \"product-sourcing\", \"supply-chain-management\", \"food-tech\",\n",
        "    \"agriculture-tech\", \"urban-planning\", \"fashion-design\", \"personal-branding\", \"career-coaching\"\n",
        "], num_pages=3)\n",
        "df.to_csv(\"freelancer_jobs_3.csv\", index=False)\n",
        "print(\"Scraping complete! Data saved to freelancer_jobs.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJLftBepDouo",
        "outputId": "7b54ddcd-d049-4665-f64b-b5da1e9ea235"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete! Data saved to freelancer_jobs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Function to scrape Freelancer.com job postings\n",
        "def scrape_freelancer_jobs(keywords, num_pages=3):\n",
        "    ua = UserAgent()\n",
        "    headers = {\"User-Agent\": ua.random}\n",
        "    jobs_list = []\n",
        "\n",
        "    if isinstance(keywords, str):  # Convert single keyword to a list\n",
        "        keywords = [keywords]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f\"https://www.freelancer.com/jobs/{keyword}/{page}/\"\n",
        "            response = requests.get(url, headers=headers)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Blocked or failed request on page {page} for keyword: {keyword}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            jobs = soup.find_all(\"div\", class_=\"JobSearchCard-item\")\n",
        "\n",
        "            for job in jobs:\n",
        "                try:\n",
        "                    title = job.find(\"a\", class_=\"JobSearchCard-primary-heading-link\").text.strip()\n",
        "                    description = job.find(\"p\", class_=\"JobSearchCard-primary-description\").text.strip()\n",
        "                    budget = job.find(\"div\", class_=\"JobSearchCard-secondary-price\").text.strip() if job.find(\"div\", class_=\"JobSearchCard-secondary-price\") else \"N/A\"\n",
        "\n",
        "                    jobs_list.append({\"Keyword\": keyword, \"Title\": title, \"Description\": description, \"Budget\": budget})\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(jobs_list)\n",
        "\n",
        "# Scrape Freelancer.com for multiple keywords\n",
        "df = scrape_freelancer_jobs([\n",
        "    \"medical-coding\", \"pharmaceutical-research\", \"clinical-trials\", \"telemedicine\",\n",
        "    \"nutrition-coaching\", \"fitness-training\", \"mental-health-counseling\", \"speech-therapy\",\n",
        "    \"physical-therapy\", \"healthcare-analytics\", \"medical-device-development\", \"health-information-management\",\n",
        "    \"online-tutoring\", \"curriculum-development\", \"educational-technology\", \"language-instruction\",\n",
        "    \"test-preparation\", \"special-education\", \"corporate-training\", \"academic-research\",\n",
        "    \"e-learning-content-development\",\n",
        "    \"architecture\", \"structural-design\", \"interior-design\", \"urban-planning\",\n",
        "    \"landscape-architecture\", \"building-information-modeling\", \"civil-infrastructure\",\n",
        "    \"residential-construction\", \"commercial-construction\", \"quantity-surveying\",\n",
        "    \"logistics-management\", \"warehouse-optimization\", \"fleet-management\", \"procurement\",\n",
        "    \"inventory-management\", \"order-fulfillment\", \"freight-forwarding\", \"last-mile-delivery\",\n",
        "    \"cold-chain-logistics\", \"supply-chain-analytics\",\n",
        "    \"broadcasting\", \"radio-hosting\", \"podcast-production\", \"film-editing\",\n",
        "    \"scriptwriting\", \"voiceover\", \"music-production\", \"dj-services\",\n",
        "    \"acting-coaching\", \"event-planning\", \"stage-lighting-design\", \"audiobook-narration\",\n",
        "], num_pages=3)\n",
        "df.to_csv(\"freelancer_jobs_4.csv\", index=False)\n",
        "print(\"Scraping complete! Data saved to freelancer_jobs.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU_wGQDhL0tJ",
        "outputId": "348d3a54-50b3-426b-9d7f-a4674b0b59b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete! Data saved to freelancer_jobs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "# Function to scrape Freelancer.com job postings\n",
        "def scrape_freelancer_jobs(keywords, num_pages=3):\n",
        "    ua = UserAgent()\n",
        "    headers = {\"User-Agent\": ua.random}\n",
        "    jobs_list = []\n",
        "\n",
        "    if isinstance(keywords, str):  # Convert single keyword to a list\n",
        "        keywords = [keywords]\n",
        "\n",
        "    for keyword in keywords:\n",
        "        for page in range(1, num_pages + 1):\n",
        "            url = f\"https://www.freelancer.com/jobs/{keyword}/{page}/\"\n",
        "            response = requests.get(url, headers=headers)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Blocked or failed request on page {page} for keyword: {keyword}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            jobs = soup.find_all(\"div\", class_=\"JobSearchCard-item\")\n",
        "\n",
        "            for job in jobs:\n",
        "                try:\n",
        "                    title = job.find(\"a\", class_=\"JobSearchCard-primary-heading-link\").text.strip()\n",
        "                    description = job.find(\"p\", class_=\"JobSearchCard-primary-description\").text.strip()\n",
        "                    budget = job.find(\"div\", class_=\"JobSearchCard-secondary-price\").text.strip() if job.find(\"div\", class_=\"JobSearchCard-secondary-price\") else \"N/A\"\n",
        "\n",
        "                    jobs_list.append({\"Keyword\": keyword, \"Title\": title, \"Description\": description, \"Budget\": budget})\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(jobs_list)\n",
        "\n",
        "# Scrape Freelancer.com for multiple keywords\n",
        "df = scrape_freelancer_jobs([\n",
        "    \"hotel-management\", \"airline-reservations\", \"travel-agency\", \"tourism-marketing\",\n",
        "    \"luxury-travel-planning\", \"airbnb-management\", \"restaurant-management\",\n",
        "    \"food-blogging\", \"menu-design\", \"hospitality-consulting\",\n",
        "    \"game-streaming\", \"esports-management\", \"game-coaching\", \"character-design\",\n",
        "    \"3d-game-animation\", \"game-localization\", \"storyboarding-for-games\",\n",
        "    \"virtual-reality-experiences\",\n",
        "    \"solar-energy-installation\", \"wind-energy-research\", \"energy-consulting\",\n",
        "    \"green-building-design\", \"carbon-footprint-reduction\", \"sustainable-packaging\",\n",
        "    \"recycling-solutions\", \"waste-management\",\n",
        "    \"aircraft-maintenance\", \"satellite-communications\", \"drone-technology\",\n",
        "    \"avionics-engineering\", \"military-strategy-analysis\", \"space-exploration-research\",\n",
        "    \"defense-cybersecurity\",\n",
        "    \"dog-walking\", \"pet-grooming\", \"veterinary-consulting\", \"pet-training\",\n",
        "    \"horse-care\", \"wildlife-photography\", \"zoo-management\",\n",
        "    \"home-decluttering\", \"feng-shui-consulting\", \"interior-decoration\",\n",
        "    \"personal-shopping\", \"home-automation\", \"smart-home-installation\",\n",
        "    \"employment-law\", \"environmental-law\", \"trademark-registration\",\n",
        "    \"litigation-support\", \"business-contract-law\", \"international-law\",\n",
        "    \"political-campaign-management\", \"public-policy-research\", \"international-relations\",\n",
        "    \"government-consulting\", \"lobbying\",\n",
        "    \"vehicle-design\", \"car-rental-business\", \"auto-repair\",\n",
        "    \"electric-vehicle-maintenance\", \"autonomous-driving-research\"\n",
        "], num_pages=3)\n",
        "df.to_csv(\"freelancer_jobs_5.csv\", index=False)\n",
        "print(\"Scraping complete! Data saved to freelancer_jobs.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pa6UjVBoMYf0",
        "outputId": "7138e0b2-ee28-48ae-d160-80bb0ee41f31"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete! Data saved to freelancer_jobs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# üìÇ Step 1: Specify the paths to your CSV files\n",
        "csv_files = [\"/content/freelancer_jobs.csv\", \"/content/freelancer_jobs_2.csv\", \"/content/freelancer_jobs_3.csv\", \"/content/freelancer_jobs_4.csv\", \"/content/freelancer_jobs_5.csv\"]  # Replace with your actual file names\n",
        "\n",
        "# üìÇ Step 2: Load all CSV files into a single DataFrame\n",
        "df_list = [pd.read_csv(file) for file in csv_files]  # Read each file into a list of DataFrames\n",
        "combined_df = pd.concat(df_list, ignore_index=True)  # Merge all into a single DataFrame\n",
        "\n",
        "# üìÇ Step 3: Save the combined dataset\n",
        "combined_df.to_csv(\"combined_freelancer_jobs_2.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ CSV files combined successfully! Data saved to combined_freelancer_jobs.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tWRkLgNACGr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd95d07-c0e3-49b3-8de3-f8b54e66978e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CSV files combined successfully! Data saved to combined_freelancer_jobs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(\"combined_freelancer_jobs_2.csv\")\n",
        "\n",
        "# Expanded list of fake job-related keywords\n",
        "fake_keywords = [\n",
        "    # üö® Scam Triggers\n",
        "    \"quick money\", \"easy cash\", \"work from home\", \"no experience needed\",\n",
        "    \"WhatsApp\", \"Telegram\", \"contact me at\", \"investment\", \"free signup\",\n",
        "    \"huge earnings\", \"earn instantly\", \"pay upfront\", \"guaranteed income\",\n",
        "    \"pyramid scheme\", \"multi-level marketing\", \"get rich quick\", \"risk-free investment\",\n",
        "    \"email me at\", \"text me\", \"direct message\", \"DM me\", \"no upfront payment required\",\n",
        "    \"advance fee\", \"Western Union\", \"Bitcoin payment\", \"no skills required\",\n",
        "    \"unlimited earnings\", \"earn $1000 daily\", \"work 1 hour a day\", \"zero effort income\",\n",
        "    \"make money fast\", \"huge profits\", \"100% profit\", \"money flipping\", \"foreign agent\",\n",
        "    \"mystery shopper\", \"financial freedom\", \"MLM opportunity\", \"no interview required\",\n",
        "    \"flexible hours unlimited pay\", \"no background check\", \"secret shopper\",\n",
        "    \"immediate start make thousands\", \"start today\", \"be your own boss\", \"home-based job\",\n",
        "    \"work remotely easy pay\", \"startup opportunity\", \"business opportunity\", \"earn extra cash\",\n",
        "\n",
        "    # üí∞ Unrealistic Salaries\n",
        "    \"earn $500 a day\", \"earn $10,000 monthly\", \"unlimited salary potential\", \"massive salary with no skills\",\n",
        "    \"instant millionaire\", \"earn $100 per click\", \"get paid $50 per survey\", \"work 2 hours per week, earn $5,000\",\n",
        "    \"turn $100 into $1,000 instantly\", \"VIP investment opportunity\", \"no work required\", \"earn while you sleep\",\n",
        "    \"guaranteed job placement\", \"make easy money\", \"passive income stream\", \"click and earn\", \"financial breakthrough\",\n",
        "    \"earn huge profits\", \"part-time, full-time, no experience\", \"unbelievable pay\", \"startup fee required\",\n",
        "    \"pay for job training\", \"instant commission\", \"easy signup bonus\", \"entry fee required\", \"get paid for nothing\",\n",
        "    \"we hire anyone\", \"join now, pay later\", \"cash rewards for joining\", \"no credit check needed\", \"ask me on WhatsApp\",\n",
        "\n",
        "    # üì© Suspicious Contact Requests\n",
        "    \"send money first\", \"contact me privately\", \"email us for quick approval\", \"telegram me for more details\",\n",
        "    \"pay before training\", \"call this number for instant hire\", \"message me directly for payment details\",\n",
        "    \"urgent hiring, no interview\", \"guaranteed approval, apply now\",\n",
        "\n",
        "    # üöÄ Fake Business Opportunities\n",
        "    \"become a reseller\", \"MLM startup bonus\", \"pyramid business model\", \"zero risk investment\",\n",
        "    \"make money in crypto instantly\", \"invest small, earn big\", \"flip your cash\", \"crypto job, no experience needed\",\n",
        "    \"trade stocks from home\", \"forex trading with no risk\",\n",
        "\n",
        "    # ‚ö† Job Descriptions with Vague Titles\n",
        "    \"mystery job\", \"classified job opportunity\", \"start today, earn today\", \"job offer without interview\",\n",
        "    \"amazing earning potential\", \"once-in-a-lifetime job\", \"hiring anyone right now\", \"trust me, this works\",\n",
        "    \"click to apply instantly\", \"no work, only profits\"\n",
        "]\n",
        "\n",
        "# Function to label fake job postings\n",
        "def detect_fake_jobs(description):\n",
        "    description = str(description).lower()  # Convert to lowercase\n",
        "    for keyword in fake_keywords:\n",
        "        if keyword in description:\n",
        "            return \"Fake\"\n",
        "    return \"Real\"\n",
        "\n",
        "# Apply the function to the dataset\n",
        "df[\"Job_Type\"] = df[\"Description\"].apply(detect_fake_jobs)\n",
        "\n",
        "# Save the updated CSV\n",
        "df.to_csv(\"filtered_freelancer_jobs.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Fake job detection complete! Check filtered_freelancer_jobs.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdgfY_RRUKKo",
        "outputId": "52659717-f6bf-4425-aa5a-9ed37be247d3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fake job detection complete! Check filtered_freelancer_jobs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "freelancer_df = pd.read_csv(\"filtered_freelancer_jobs.csv\")\n",
        "sriraj_df = pd.read_csv(\"/content/Sriraj's fake_job_postings.csv\")\n",
        "\n",
        "# Filter only fake jobs\n",
        "fake_jobs = sriraj_df[sriraj_df['fraudulent'] == 1]\n",
        "\n",
        "# Select and rename columns to match freelancer format\n",
        "fake_jobs_filtered = fake_jobs[['description', 'function', 'title']].copy()\n",
        "fake_jobs_filtered.rename(columns={\n",
        "    'description': 'Description',\n",
        "    'function': 'Keyword',\n",
        "    'title': 'Title'\n",
        "}, inplace=True)\n",
        "\n",
        "# Add missing columns\n",
        "fake_jobs_filtered['Budget'] = 'N/A'\n",
        "fake_jobs_filtered['Job_Type'] = 'Fake'\n",
        "\n",
        "# Reorder columns\n",
        "fake_jobs_filtered = fake_jobs_filtered[['Keyword', 'Title', 'Description', 'Budget', 'Job_Type']]\n",
        "\n",
        "# Combine with original freelancer dataset\n",
        "updated_freelancer_df = pd.concat([freelancer_df, fake_jobs_filtered], ignore_index=True)\n",
        "\n",
        "# Save the updated dataset\n",
        "updated_freelancer_df.to_csv(\"updated_freelancer_jobs.csv\", index=False)\n",
        "\n",
        "print(\"Updated dataset saved as 'updated_freelancer_jobs.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22uhQhROkKsz",
        "outputId": "0da8fe04-eb26-4b03-b191-907f02e17e82"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated dataset saved as 'updated_freelancer_jobs.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv(\"/content/updated_freelancer_jobs.csv\")\n",
        "\n",
        "# Step 2: Separate real and fake jobs\n",
        "real_jobs = df[df['Job_Type'] == 'Real']\n",
        "fake_jobs = df[df['Job_Type'] == 'Fake']\n",
        "\n",
        "# Step 3: Sample 10,000 real jobs\n",
        "real_jobs_sampled = real_jobs.sample(n=10000, random_state=42)\n",
        "\n",
        "# Step 4: Calculate how many more fake jobs we need\n",
        "current_fake = fake_jobs.shape[0]\n",
        "target_fake = 5000\n",
        "needed_fake = target_fake - current_fake\n",
        "\n",
        "# Step 5: Generate synthetic fake jobs via resampling\n",
        "synthetic_fake_jobs = resample(\n",
        "    fake_jobs,\n",
        "    replace=True,\n",
        "    n_samples=needed_fake,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Optional: Label synthetic entries if needed\n",
        "# synthetic_fake_jobs['synthetic'] = True\n",
        "# fake_jobs['synthetic'] = False\n",
        "\n",
        "# Step 6: Combine real and extended fake jobs\n",
        "final_fake_jobs = pd.concat([fake_jobs, synthetic_fake_jobs], ignore_index=True)\n",
        "final_df = pd.concat([real_jobs_sampled, final_fake_jobs], ignore_index=True)\n",
        "\n",
        "# Step 7: Save the final balanced dataset\n",
        "final_df.to_csv(\"balanced_freelancer_jobs.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Dataset saved as 'balanced_freelancer_jobs.csv' with:\")\n",
        "print(f\"- {real_jobs_sampled.shape[0]} real jobs\")\n",
        "print(f\"- {final_fake_jobs.shape[0]} fake jobs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOWKecxgk3w5",
        "outputId": "8e898f9c-25e3-4609-c54a-4d17030dcb34"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset saved as 'balanced_freelancer_jobs.csv' with:\n",
            "- 10000 real jobs\n",
            "- 5000 fake jobs\n"
          ]
        }
      ]
    }
  ]
}